{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis - Draft Version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import sklearn \n",
    "import itertools \n",
    "import os \n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif as MI, SelectPercentile\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score, roc_auc_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist.info import INFO, HOMEPAGE, DEFAULT_ROOT\n",
    "\n",
    "try:\n",
    "    from collections.abc import Sequence\n",
    "except ImportError:\n",
    "    from collections import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Analysis \n",
    "\n",
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries needed \n",
    "datasets = {}\n",
    "class_number = {}\n",
    "features = {}\n",
    "labels = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to name datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_namer(input_name, suffix, size=''): #size as optional parameter!\n",
    "    global string\n",
    "    if size != '':\n",
    "        string = f\"{input_name}_{suffix}_{size}\"\n",
    "    else:\n",
    "        string = f\"{input_name}_{suffix}\"\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate the MedMNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medmnist_generator(data_flag, split, size):\n",
    "\n",
    "    # Taken from MedMNIST v2 GitHub\n",
    "    info = INFO[data_flag]\n",
    "    task = info['task']\n",
    "    n_channels = info['n_channels']\n",
    "    n_classes = len(info['label'])\n",
    "\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    # Preprocessing\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5])\n",
    "        ])\n",
    "    \n",
    "    global class_number\n",
    "    \n",
    "    \n",
    "    # Creates dictionary with number of classes for all features and labels.\n",
    "    name = dataset_namer(data_flag, size)\n",
    "    class_entry = {name: n_classes}\n",
    "    class_number.update(class_entry)\n",
    "    \n",
    "    # Use of Dataset_Namer function to encode outputs\n",
    "    ds_name = dataset_namer(data_flag, split, size)\n",
    "    \n",
    "    global datasets\n",
    "    \n",
    "    # Splits each dataset into training, validation and testing dataset. \n",
    "    value = DataClass(split=split, size=int(size), transform=data_transform,download=True) \n",
    "    entry = {ds_name: value}\n",
    "    datasets.update(entry)\n",
    "    \n",
    "    globals()[ds_name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retrieve variable name as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_name(input_var):\n",
    "    for name, var in globals().items():\n",
    "        if var is input_var:\n",
    "            return name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract features and labels from MedMNIST image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts Features and Labels\n",
    "def features_labels(key, value):   \n",
    "\n",
    "    # Extract features and transform to torch\n",
    "    X = value.imgs\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    X = torch.from_numpy(X)\n",
    "    \n",
    "    # Extract labels and transform to torch\n",
    "    y = value.labels\n",
    "    y = np.ravel(y)\n",
    "    y = torch.from_numpy(y)\n",
    "    \n",
    "    # Name feature and labels datasets\n",
    "    f_name = dataset_namer(key, \"features\", '')\n",
    "    l_name = dataset_namer(key, \"labels\", '')\n",
    "    \n",
    "    globals()[f_name] = X\n",
    "    globals()[l_name] = y\n",
    "    \n",
    "    global features, labels\n",
    "\n",
    "    f_entry = {f_name: X}\n",
    "    features.update(f_entry) \n",
    "\n",
    "    l_entry = {l_name: y}\n",
    "    labels.update(l_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split dictionaries by specified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split dictionaries\n",
    "def dict_split(dictionary, split):\n",
    "    \n",
    "    new_dict = {}\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        if split in key:\n",
    "            name = dataset_namer(split, get_var_name(dictionary))\n",
    "            new_dict[key] = value\n",
    "            globals()[name] = new_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to transform data into dataloader form for deep learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform data into dataloader form for deep learning\n",
    "def data_loader(name, batch_size):\n",
    "    name = dataset_namer(name, \"loader\", '')\n",
    "    if 'train' in name:\n",
    "        globals()[name] = data.DataLoader(dataset = name, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    else: \n",
    "        globals()[name] = data.DataLoader(dataset = name, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for principal component analysis (linear and non-linear kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA/Kernel PCA\n",
    "def pca(data, normalise='Yes', kernel='No',kernel_type='linear'):\n",
    "    \n",
    "    ds_name = variable_name(data)\n",
    "    \n",
    "    if kernel == 'No':\n",
    "    \n",
    "        if normalise == 'Yes':\n",
    "\n",
    "            name = dataset_namer(ds_name, \"normalised_pca\", '')\n",
    "\n",
    "            def pca_normalise(data):\n",
    "\n",
    "                data = StandardScaler().fit_transform(data)\n",
    "                feature_cols = ['feature'+str(i) for i in range(data.shape[1])]    \n",
    "                normalised_features = pd.DataFrame(data,columns=feature_cols)\n",
    "                data = normalised_features\n",
    "\n",
    "                return data\n",
    "\n",
    "            data = pca_normalise(data)\n",
    "\n",
    "        elif normalise == 'No': \n",
    "            name = dataset_namer(ds_name, \"pca\", '')\n",
    "\n",
    "        else: \n",
    "            print(\"ERROR: Invalid input to normalise parameter. Please choose 'Yes' or 'No'.\")\n",
    "\n",
    "\n",
    "        pca = PCA()\n",
    "        principalComponents = pca.fit_transform(data)\n",
    "        pca_cols = ['pc'+str(i) for i in range(principalComponents.shape[1])]\n",
    "\n",
    "        value = pd.DataFrame(data = principalComponents, columns = pca_cols)\n",
    "        entry = {name: value}\n",
    "        datasets.update(entry)\n",
    "        globals()[name] = value\n",
    "        \n",
    "    elif kernel == 'Yes':\n",
    "    \n",
    "        if normalise == 'Yes':\n",
    "\n",
    "            name = dataset_namer(ds_name, \"normalised_kernel_pca\", '')\n",
    "\n",
    "            def pca_normalise(data):\n",
    "\n",
    "                data = StandardScaler().fit_transform(data)\n",
    "                feature_cols = ['feature'+str(i) for i in range(data.shape[1])]    \n",
    "                normalised_features = pd.DataFrame(data,columns=feature_cols)\n",
    "                data = normalised_features\n",
    "\n",
    "                return data\n",
    "\n",
    "            data = pca_normalise(data)\n",
    "\n",
    "        elif normalise == 'No': \n",
    "            name = dataset_namer(ds_name, \"kernel_pca\", '')\n",
    "\n",
    "        else: \n",
    "            print(\"ERROR: Invalid input to normalise parameter. Please choose 'Yes' or 'No'.\")\n",
    "\n",
    "\n",
    "        kernel_pca = KernelPCA(kernel=kernel_type)\n",
    "        kernel_principalComponents = kernel_pca.fit_transform(data)\n",
    "        kernel_pca_cols = ['pc'+str(i) for i in range(kernel_principalComponents.shape[1])]\n",
    "\n",
    "        value = pd.DataFrame(data = kernel_principalComponents, columns = kernel_pca_cols)\n",
    "        entry = {name: value}\n",
    "        datasets.update(entry)\n",
    "        globals()[name] = value\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: Invalid input to kernel parameter. Please choose 'Yes' or 'No'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data \n",
    "\n",
    "### Specifying Function Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Data Flags and Data Splits\n",
    "data_flag = ('pathmnist','dermamnist','breastmnist')\n",
    "split = ('train','test','val')\n",
    "size = (28,64,128,224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loop to Generate Data\n",
    "for a, b, c in itertools.product(sorted(data_flag), split, size): \n",
    "    medmnist_generator(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show information for PathMNIST\n",
    "print(pathmnist_train_28)\n",
    "print(pathmnist_train_64)\n",
    "print(pathmnist_train_128)\n",
    "print(pathmnist_train_224)\n",
    "\n",
    "# Show information for DermaMNIST\n",
    "print(dermamnist_train_28)\n",
    "print(dermamnist_train_64)\n",
    "print(dermamnist_train_128)\n",
    "print(dermamnist_train_224)\n",
    "\n",
    "# Show information for BreastMNIST\n",
    "print(breastmnist_train_28)\n",
    "print(breastmnist_train_64)\n",
    "print(breastmnist_train_128)\n",
    "print(breastmnist_train_224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 7x7 grid (49 samples) original low resolution images\n",
    "##breastmnist_train_28.montage(length=7).save(\"breastmnist_lowres_sample.jpeg\")\n",
    "##pathmnist_train_28.montage(length=7).save(\"pathmnist_lowres_sample.jpeg\")\n",
    "##dermamnist_train_28.montage(length=7).save(\"dermamnist_lowres_sample.jpeg\")\n",
    " \n",
    "# Generate 7x7 grid (49 samples) highest resolution images\n",
    "##breastmnist_train_224.montage(length=7).save(\"breastmnist_highres_sample.jpeg\")\n",
    "##pathmnist_train_224.montage(length=7).save(\"pathmnist_highres_sample.jpeg\")\n",
    "##dermamnist_train_224.montage(length=7).save(\"dermamnist_highres_sample.jpeg\")\n",
    "\n",
    "# Resolution Comparison \n",
    "##dermamnist_train_28.montage(length=7).save(\"res_comp1.jpeg\")\n",
    "##dermamnist_train_64.montage(length=7).save(\"res_comp2.jpeg\")\n",
    "##dermamnist_train_128.montage(length=7).save(\"res_comp3.jpeg\")\n",
    "##dermamnist_train_224.montage(length=7).save(\"res_comp4.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features and Labels from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to extract features and labels over whole dictionary\n",
    "for key, value in datasets.items():\n",
    "    features_labels(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split features and labels by train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels into train/test/val\n",
    "for i in split:\n",
    "    dict_split(features, i)\n",
    "    dict_split(labels, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "### Quantitites needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing quantities needed\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning \n",
    "\n",
    "### Transform data into dataloader form for deep learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function over all our datasets\n",
    "for key in datasets.keys():\n",
    "    data_loader(key, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
